{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一步：数据处理"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （1）数据读取\n",
    "通过如下代码读入数据，了解下波士顿房价的数据集结构，数据存放在本地目录下housing.data文件中。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.320e-03, 1.800e+01, 2.310e+00, ..., 3.969e+02, 7.880e+00,\n",
       "       1.190e+01])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 从文件里面把数据读取出来\n",
    "data_file = \"./dataset/housing.data\"\n",
    "data = np.fromfile(file=data_file, sep=\" \")\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （2）数据形状变换\n",
    "由于读入的原始数据是1维的，所有数据都连在一起。因此需要我们将数据的形状进行变换，形成一个2维的矩阵，每行为一个数据样本（14个值），每个数据样本包含13个x（影响房价的特征）和一个 y（该类型房屋的均价）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 3.9690e+02, 4.9800e+00,\n",
       "        2.4000e+01],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 3.9690e+02, 9.1400e+00,\n",
       "        2.1600e+01],\n",
       "       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 3.9283e+02, 4.0300e+00,\n",
       "        3.4700e+01],\n",
       "       ...,\n",
       "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 3.9690e+02, 5.6400e+00,\n",
       "        2.3900e+01],\n",
       "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 3.9345e+02, 6.4800e+00,\n",
       "        2.2000e+01],\n",
       "       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 3.9690e+02, 7.8800e+00,\n",
       "        1.1900e+01]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读入之后的数据被转化成1维array，其中array的第0-13项是第一条数据，第14-27项是第二条数据，以此类推.... \n",
    "# 这里对原始数据做reshape，变成N x 14的形式\n",
    "feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE','DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "feature_num = len(feature_names) # 获取一行里面有多少个列，13+1=14\n",
    "data = data.reshape([data.shape[0] // feature_num, feature_num]) # data.shape[0] // feature_num 得出行数，后面是列数\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14,)\n",
      "[6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01\n",
      " 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00 2.400e+01]\n"
     ]
    }
   ],
   "source": [
    "# 这里可以获取一条数据进行预览,验证一下里面的列数和数据是否正常\n",
    "print(data[0].shape)\n",
    "print(data[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3）数据集划分\n",
    "将数据集划分成训练集和测试集，其中训练集用于确定模型的参数，测试集用于评判模型的效果。为什么要对数据集进行拆分，\n",
    "在本案例中，我们将80%的数据用作训练集，20%用作测试集，实现代码如下。通过打印训练集的形状，可以发现共有404个样本，每个样本含有13个特征和1个预测值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(404, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = 0.8 # 拆分比例，80%的训练集，20%的测试集\n",
    "print(data.shape[0]) #数据为506行\n",
    "offset = int(data.shape[0] * ratio) # 然后乘以0.8 就是求出训练集的具体行数\n",
    "training_data = data[:offset] # 然后分割训练数据集\n",
    "training_data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （4）数据归一化处理\n",
    "对每个特征进行归一化处理，使得每个特征的取值缩放到0~1之间。这样做有两个好处：\n",
    "- 一是模型训练更高效；\n",
    "- 二是特征前的权重大小可以代表该变量对预测结果的贡献度（因为每个特征值本身的范围相同）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 1.80000000e-01, 7.34418420e-02, ...,\n",
       "        1.00000000e+00, 8.96799117e-02, 4.22222222e-01],\n",
       "       [2.35922539e-04, 0.00000000e+00, 2.62405717e-01, ...,\n",
       "        1.00000000e+00, 2.04470199e-01, 3.68888889e-01],\n",
       "       [2.35697744e-04, 0.00000000e+00, 2.62405717e-01, ...,\n",
       "        9.87519166e-01, 6.34657837e-02, 6.60000000e-01],\n",
       "       ...,\n",
       "       [6.11892474e-04, 0.00000000e+00, 4.55339420e-01, ...,\n",
       "        1.00000000e+00, 1.07891832e-01, 4.20000000e-01],\n",
       "       [1.16072990e-03, 0.00000000e+00, 4.55339420e-01, ...,\n",
       "        9.89420423e-01, 1.31070640e-01, 3.77777778e-01],\n",
       "       [4.61841693e-04, 0.00000000e+00, 4.55339420e-01, ...,\n",
       "        1.00000000e+00, 1.69701987e-01, 1.53333333e-01]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算train数据集的最大值，最小值\n",
    "# axis=0 表示获取每一列的最大值 最小值\n",
    "maximums, minimums = training_data.max(axis=0),training_data.min(axis=0)\n",
    "print(len(minimums))  # 14个\n",
    "# 对数据进行归一化处理\n",
    "for i in range(feature_num):\n",
    "    # 进行14次循环，然后对所有样本的每列进行统一归一化数据处理\n",
    "    data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （5）封装成load data函数\n",
    "将上面的步骤合并在一起，变成一个数据处理的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # 从文件导入数据\n",
    "    datafile = './dataset/housing.data'\n",
    "    data = np.fromfile(datafile, sep=' ')\n",
    "\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "    feature_num = len(feature_names)\n",
    "\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\n",
    "\n",
    "    # 将原数据集拆分成训练集和测试集\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\n",
    "    # 测试集和训练集必须是没有交集的\n",
    "    ratio = 0.8\n",
    "    offset = int(data.shape[0] * ratio)\n",
    "    training_data = data[:offset]\n",
    "\n",
    "    # 计算训练集的最大值，最小值\n",
    "    maximums, minimums = training_data.max(axis=0), training_data.min(axis=0)\n",
    "\n",
    "    # 对数据进行归一化处理\n",
    "    for i in range(feature_num):\n",
    "        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])\n",
    "\n",
    "    # 训练集和测试集的划分比例\n",
    "    training_data = data[:offset]\n",
    "    test_data = data[offset:]\n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.00000000e+00, 1.80000000e-01, 7.34418420e-02, ...,\n",
       "         2.87234043e-01, 1.00000000e+00, 8.96799117e-02],\n",
       "        [2.35922539e-04, 0.00000000e+00, 2.62405717e-01, ...,\n",
       "         5.53191489e-01, 1.00000000e+00, 2.04470199e-01],\n",
       "        [2.35697744e-04, 0.00000000e+00, 2.62405717e-01, ...,\n",
       "         5.53191489e-01, 9.87519166e-01, 6.34657837e-02],\n",
       "        ...,\n",
       "        [1.59940420e-01, 0.00000000e+00, 7.00277888e-01, ...,\n",
       "         8.08510638e-01, 1.00000000e+00, 5.12969095e-01],\n",
       "        [1.07782432e-01, 0.00000000e+00, 7.00277888e-01, ...,\n",
       "         8.08510638e-01, 9.36246550e-01, 5.12693157e-01],\n",
       "        [2.78694093e-01, 0.00000000e+00, 7.00277888e-01, ...,\n",
       "         8.08510638e-01, 1.00000000e+00, 4.97792494e-01]]),\n",
       " array([[0.42222222],\n",
       "        [0.36888889],\n",
       "        [0.66      ],\n",
       "        [0.63111111],\n",
       "        [0.69333333],\n",
       "        [0.52666667],\n",
       "        [0.39777778],\n",
       "        [0.49111111],\n",
       "        [0.25555556],\n",
       "        [0.30888889],\n",
       "        [0.22222222],\n",
       "        [0.30888889],\n",
       "        [0.37111111],\n",
       "        [0.34222222],\n",
       "        [0.29333333],\n",
       "        [0.33111111],\n",
       "        [0.40222222],\n",
       "        [0.27777778],\n",
       "        [0.33777778],\n",
       "        [0.29333333],\n",
       "        [0.19111111],\n",
       "        [0.32444444],\n",
       "        [0.22666667],\n",
       "        [0.21111111],\n",
       "        [0.23555556],\n",
       "        [0.19777778],\n",
       "        [0.25777778],\n",
       "        [0.21777778],\n",
       "        [0.29777778],\n",
       "        [0.35555556],\n",
       "        [0.17111111],\n",
       "        [0.21111111],\n",
       "        [0.18222222],\n",
       "        [0.18      ],\n",
       "        [0.18888889],\n",
       "        [0.30888889],\n",
       "        [0.33333333],\n",
       "        [0.35555556],\n",
       "        [0.43777778],\n",
       "        [0.57333333],\n",
       "        [0.66444444],\n",
       "        [0.48      ],\n",
       "        [0.45111111],\n",
       "        [0.43777778],\n",
       "        [0.36      ],\n",
       "        [0.31777778],\n",
       "        [0.33333333],\n",
       "        [0.25777778],\n",
       "        [0.20888889],\n",
       "        [0.32      ],\n",
       "        [0.32666667],\n",
       "        [0.34444444],\n",
       "        [0.44444444],\n",
       "        [0.40888889],\n",
       "        [0.30888889],\n",
       "        [0.67555556],\n",
       "        [0.43777778],\n",
       "        [0.59111111],\n",
       "        [0.40666667],\n",
       "        [0.32444444],\n",
       "        [0.30444444],\n",
       "        [0.24444444],\n",
       "        [0.38222222],\n",
       "        [0.44444444],\n",
       "        [0.62222222],\n",
       "        [0.41111111],\n",
       "        [0.32      ],\n",
       "        [0.37777778],\n",
       "        [0.27555556],\n",
       "        [0.35333333],\n",
       "        [0.42666667],\n",
       "        [0.37111111],\n",
       "        [0.39555556],\n",
       "        [0.40888889],\n",
       "        [0.42444444],\n",
       "        [0.36444444],\n",
       "        [0.33333333],\n",
       "        [0.35111111],\n",
       "        [0.36      ],\n",
       "        [0.34      ],\n",
       "        [0.51111111],\n",
       "        [0.42      ],\n",
       "        [0.44      ],\n",
       "        [0.39777778],\n",
       "        [0.42      ],\n",
       "        [0.48      ],\n",
       "        [0.38888889],\n",
       "        [0.38222222],\n",
       "        [0.41333333],\n",
       "        [0.52666667],\n",
       "        [0.39111111],\n",
       "        [0.37777778],\n",
       "        [0.39777778],\n",
       "        [0.44444444],\n",
       "        [0.34666667],\n",
       "        [0.52      ],\n",
       "        [0.36444444],\n",
       "        [0.74888889],\n",
       "        [0.86222222],\n",
       "        [0.62666667],\n",
       "        [0.5       ],\n",
       "        [0.47777778],\n",
       "        [0.30222222],\n",
       "        [0.31777778],\n",
       "        [0.33555556],\n",
       "        [0.32222222],\n",
       "        [0.32222222],\n",
       "        [0.34222222],\n",
       "        [0.32888889],\n",
       "        [0.32      ],\n",
       "        [0.37111111],\n",
       "        [0.39555556],\n",
       "        [0.30666667],\n",
       "        [0.30444444],\n",
       "        [0.3       ],\n",
       "        [0.29555556],\n",
       "        [0.36      ],\n",
       "        [0.31555556],\n",
       "        [0.34222222],\n",
       "        [0.31777778],\n",
       "        [0.37777778],\n",
       "        [0.34      ],\n",
       "        [0.34444444],\n",
       "        [0.27333333],\n",
       "        [0.30666667],\n",
       "        [0.36444444],\n",
       "        [0.23777778],\n",
       "        [0.24888889],\n",
       "        [0.28888889],\n",
       "        [0.20666667],\n",
       "        [0.31555556],\n",
       "        [0.32444444],\n",
       "        [0.4       ],\n",
       "        [0.29777778],\n",
       "        [0.23555556],\n",
       "        [0.29111111],\n",
       "        [0.27555556],\n",
       "        [0.26888889],\n",
       "        [0.18444444],\n",
       "        [0.28444444],\n",
       "        [0.2       ],\n",
       "        [0.20888889],\n",
       "        [0.18666667],\n",
       "        [0.23555556],\n",
       "        [0.15111111],\n",
       "        [0.19555556],\n",
       "        [0.23555556],\n",
       "        [0.21333333],\n",
       "        [0.28444444],\n",
       "        [0.23111111],\n",
       "        [0.36666667],\n",
       "        [0.32444444],\n",
       "        [0.22888889],\n",
       "        [0.32      ],\n",
       "        [0.26666667],\n",
       "        [0.23555556],\n",
       "        [0.18      ],\n",
       "        [0.80666667],\n",
       "        [0.42888889],\n",
       "        [0.40666667],\n",
       "        [0.48888889],\n",
       "        [1.        ],\n",
       "        [1.        ],\n",
       "        [1.        ],\n",
       "        [0.39333333],\n",
       "        [0.44444444],\n",
       "        [1.        ],\n",
       "        [0.41777778],\n",
       "        [0.41777778],\n",
       "        [0.38444444],\n",
       "        [0.27555556],\n",
       "        [0.31333333],\n",
       "        [0.40222222],\n",
       "        [0.41333333],\n",
       "        [0.39111111],\n",
       "        [0.54222222],\n",
       "        [0.40444444],\n",
       "        [0.43555556],\n",
       "        [0.55333333],\n",
       "        [0.71555556],\n",
       "        [0.77333333],\n",
       "        [0.69333333],\n",
       "        [0.73111111],\n",
       "        [0.61111111],\n",
       "        [0.47555556],\n",
       "        [0.54666667],\n",
       "        [1.        ],\n",
       "        [0.6       ],\n",
       "        [0.55111111],\n",
       "        [0.66444444],\n",
       "        [0.71111111],\n",
       "        [0.56666667],\n",
       "        [0.69777778],\n",
       "        [0.58      ],\n",
       "        [0.53555556],\n",
       "        [1.        ],\n",
       "        [0.62888889],\n",
       "        [0.56222222],\n",
       "        [0.65777778],\n",
       "        [0.66444444],\n",
       "        [0.62      ],\n",
       "        [0.42444444],\n",
       "        [0.82888889],\n",
       "        [0.96666667],\n",
       "        [1.        ],\n",
       "        [0.39111111],\n",
       "        [0.43111111],\n",
       "        [0.38888889],\n",
       "        [0.43111111],\n",
       "        [0.33333333],\n",
       "        [0.37111111],\n",
       "        [0.31777778],\n",
       "        [0.38666667],\n",
       "        [0.51333333],\n",
       "        [0.41555556],\n",
       "        [0.44444444],\n",
       "        [0.40666667],\n",
       "        [0.52666667],\n",
       "        [0.36666667],\n",
       "        [0.4       ],\n",
       "        [0.48222222],\n",
       "        [0.37111111],\n",
       "        [0.5       ],\n",
       "        [0.55777778],\n",
       "        [0.88444444],\n",
       "        [1.        ],\n",
       "        [0.72444444],\n",
       "        [0.59111111],\n",
       "        [0.92666667],\n",
       "        [0.58888889],\n",
       "        [0.42888889],\n",
       "        [0.59333333],\n",
       "        [0.81555556],\n",
       "        [0.96222222],\n",
       "        [0.53333333],\n",
       "        [0.42222222],\n",
       "        [0.44666667],\n",
       "        [0.58888889],\n",
       "        [0.41555556],\n",
       "        [0.40666667],\n",
       "        [0.37777778],\n",
       "        [0.33555556],\n",
       "        [0.38222222],\n",
       "        [0.41555556],\n",
       "        [0.28      ],\n",
       "        [0.3       ],\n",
       "        [0.42888889],\n",
       "        [0.34444444],\n",
       "        [0.43333333],\n",
       "        [0.47111111],\n",
       "        [0.43111111],\n",
       "        [0.44      ],\n",
       "        [0.54666667],\n",
       "        [0.84      ],\n",
       "        [0.37555556],\n",
       "        [0.35333333],\n",
       "        [0.86666667],\n",
       "        [1.        ],\n",
       "        [0.68888889],\n",
       "        [0.55777778],\n",
       "        [0.64      ],\n",
       "        [0.84666667],\n",
       "        [0.97333333],\n",
       "        [0.57777778],\n",
       "        [0.7       ],\n",
       "        [0.39555556],\n",
       "        [0.57111111],\n",
       "        [1.        ],\n",
       "        [0.85555556],\n",
       "        [0.34888889],\n",
       "        [0.35777778],\n",
       "        [0.44888889],\n",
       "        [0.43111111],\n",
       "        [0.67111111],\n",
       "        [0.60888889],\n",
       "        [0.6       ],\n",
       "        [0.62666667],\n",
       "        [0.62444444],\n",
       "        [0.53555556],\n",
       "        [0.66888889],\n",
       "        [0.89777778],\n",
       "        [0.67555556],\n",
       "        [0.91111111],\n",
       "        [1.        ],\n",
       "        [0.60444444],\n",
       "        [0.37777778],\n",
       "        [0.33555556],\n",
       "        [0.40444444],\n",
       "        [0.38444444],\n",
       "        [0.44      ],\n",
       "        [0.52222222],\n",
       "        [0.71777778],\n",
       "        [0.50888889],\n",
       "        [0.42      ],\n",
       "        [0.37111111],\n",
       "        [0.52444444],\n",
       "        [0.49111111],\n",
       "        [0.34      ],\n",
       "        [0.38888889],\n",
       "        [0.53333333],\n",
       "        [0.44      ],\n",
       "        [0.37777778],\n",
       "        [0.47555556],\n",
       "        [0.62444444],\n",
       "        [0.69111111],\n",
       "        [0.52      ],\n",
       "        [0.63111111],\n",
       "        [0.51555556],\n",
       "        [0.39555556],\n",
       "        [0.34      ],\n",
       "        [0.24666667],\n",
       "        [0.38      ],\n",
       "        [0.32      ],\n",
       "        [0.36888889],\n",
       "        [0.41777778],\n",
       "        [0.24888889],\n",
       "        [0.28444444],\n",
       "        [0.32888889],\n",
       "        [0.40222222],\n",
       "        [0.35555556],\n",
       "        [0.41777778],\n",
       "        [0.40222222],\n",
       "        [0.34222222],\n",
       "        [0.3       ],\n",
       "        [0.44444444],\n",
       "        [0.43555556],\n",
       "        [0.4       ],\n",
       "        [0.38222222],\n",
       "        [0.31777778],\n",
       "        [0.39111111],\n",
       "        [0.32888889],\n",
       "        [0.26888889],\n",
       "        [0.32      ],\n",
       "        [0.38222222],\n",
       "        [0.34888889],\n",
       "        [0.35777778],\n",
       "        [0.32222222],\n",
       "        [0.3       ],\n",
       "        [0.34666667],\n",
       "        [0.31111111],\n",
       "        [0.30444444],\n",
       "        [0.61555556],\n",
       "        [0.25555556],\n",
       "        [0.42      ],\n",
       "        [0.58222222],\n",
       "        [0.27777778],\n",
       "        [0.27111111],\n",
       "        [0.40222222],\n",
       "        [0.43333333],\n",
       "        [0.48      ],\n",
       "        [0.39777778],\n",
       "        [0.42444444],\n",
       "        [0.30222222],\n",
       "        [0.55777778],\n",
       "        [0.29333333],\n",
       "        [0.34666667],\n",
       "        [0.28444444],\n",
       "        [0.37111111],\n",
       "        [0.39333333],\n",
       "        [0.39111111],\n",
       "        [0.44444444],\n",
       "        [0.33111111],\n",
       "        [0.35111111],\n",
       "        [0.26222222],\n",
       "        [0.37555556],\n",
       "        [0.5       ],\n",
       "        [0.37555556],\n",
       "        [0.40222222],\n",
       "        [1.        ],\n",
       "        [1.        ],\n",
       "        [1.        ],\n",
       "        [1.        ],\n",
       "        [1.        ],\n",
       "        [0.19555556],\n",
       "        [0.19555556],\n",
       "        [0.22222222],\n",
       "        [0.19777778],\n",
       "        [0.18444444],\n",
       "        [0.18      ],\n",
       "        [0.11555556],\n",
       "        [0.12      ],\n",
       "        [0.13111111],\n",
       "        [0.14      ],\n",
       "        [0.16222222],\n",
       "        [0.08444444],\n",
       "        [0.04888889],\n",
       "        [0.12222222],\n",
       "        [0.05333333],\n",
       "        [0.11555556],\n",
       "        [0.14444444],\n",
       "        [0.22444444],\n",
       "        [0.40444444],\n",
       "        [0.10444444],\n",
       "        [0.19555556],\n",
       "        [0.17111111],\n",
       "        [0.18      ],\n",
       "        [0.16666667],\n",
       "        [0.07777778],\n",
       "        [0.        ],\n",
       "        [0.02888889],\n",
       "        [0.01333333],\n",
       "        [0.04888889],\n",
       "        [0.15777778],\n",
       "        [0.07333333]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data,test_data = load_data()\n",
    "x = training_data[:,:-1] # 获取第一条的前面13个值\n",
    "y = training_data[:,-1:] # 获取第一条的最后一个值\n",
    "x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二步：模型设计\n",
    "模型设计是深度学习模型关键要素之一，也称为网络结构设计，相当于模型的假设空间，即实现模型“前向计算”（从输入到输出）的过程。\n",
    "如果将输入特征和输出预测值均以向量表示，输入特征x有13个向量，y有1个向量,shape=[1,13]那么参数权重的形状是[13,1]假设我们以如下任意数字赋值参数做初始化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1],\n",
       "       [ 0.2],\n",
       "       [ 0.3],\n",
       "       [ 0.4],\n",
       "       [ 0.5],\n",
       "       [ 0.6],\n",
       "       [ 0.7],\n",
       "       [ 0.8],\n",
       "       [-0.1],\n",
       "       [-0.2],\n",
       "       [-0.3],\n",
       "       [-0.4],\n",
       "       [ 0. ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, -0.1, -0.2, -0.3, -0.4, 0.0]\n",
    "w = np.array(w).reshape([13, 1])\n",
    "w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取出第1条样本数据，观察样本的特征向量与参数向量相乘的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69474855]\n"
     ]
    }
   ],
   "source": [
    "x1 = x[0]\n",
    "t = np.dot(x1, w) # 进行矩阵乘法计算\n",
    "print(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写一个简单版本的网络类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWork(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，\n",
    "        # 此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1) # 随机生成一个权重W 然后形状是 [13,1]\n",
    "        self.b = 0.  # b 偏置值\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于Network类的定义，模型的计算过程如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.39362982]\n",
      "[3.88644793]\n"
     ]
    }
   ],
   "source": [
    "net = NetWork(13)\n",
    "x1 = x[0]\n",
    "y1 = y[0]\n",
    "z = net.forward(x1)\n",
    "print(z)\n",
    "\n",
    "# 计算损失函数\n",
    "Loss = (y1 - z)*(y1 - z)\n",
    "print(Loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三步：训练配置\n",
    "模型设计完成后，需要通过训练配置寻找模型的最优值，即通过损失函数来衡量模型的好坏。训练配置也是深度学习模型关键要素之一。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Network类下面添加损失函数的代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWork2(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        cost = error * error\n",
    "        cost = np.mean(cost)\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict:  [[2.39362982]\n",
      " [2.46752393]\n",
      " [2.02483479]]\n",
      "loss: 3.384496992612791\n"
     ]
    }
   ],
   "source": [
    "net = NetWork2(13) # 向前计算获取预测值\n",
    "# 此处可以一次性计算多个样本的预测值和损失函数\n",
    "x1 = x[0:3] # 传三个样本值\n",
    "y1 = y[0:3] # 获取前面三个真实值\n",
    "z = net.forward(x1) # 传入三个样本，获取模型的预测值\n",
    "print('predict: ', z)\n",
    "loss = net.loss(z, y1) # 计算预测值和真实值之间的损失\n",
    "print('loss:', loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四步：训练过程\n",
    "上述计算过程描述了如何构建神经网络，通过神经网络完成预测值和损失函数的计算。接下来介绍如何求解参数w和b的数值，这个过程也称为模型训练过程。训练过程是深度学习模型的关键要素之一，其目标是让定义的损失函数尽可能的小，也就是说找到一个参数解w和b，使得损失函数取得极小值。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （1）梯度下降法"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在现实中存在大量的函数正向求解容易，但反向求解较难，被称为单向函数，这种函数在密码学中有大量的应用。密码锁的特点是可以迅速判断一个密钥是否是正确的(已知x，求y很容易)，但是即使获取到密码锁系统，无法破解出正确的密钥是什么（已知y，求x很难）。神经网络模型的损失函数就是这样的单向函数，反向求解并不容易。\n",
    "\n",
    "说人话：就是不断的调节权重w和偏执b的值，每次都向梯度最小的地方移动一点点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 [0.         0.18       0.07344184 0.         0.31481481 0.57750527\n",
      " 0.64160659 0.26920314 0.         0.22755741 0.28723404 1.\n",
      " 0.08967991], shape (13,)\n",
      "y1 [0.42222222], shape (1,)\n",
      "z1 [2.39362982], shape (1,)\n"
     ]
    }
   ],
   "source": [
    "x1 = x[0] # 取一个样本\n",
    "y1 = y[0] # 取真实值\n",
    "z1 = net.forward(x1) # 获取预测值\n",
    "print('x1 {}, shape {}'.format(x1, x1.shape))\n",
    "print('y1 {}, shape {}'.format(y1, y1.shape))\n",
    "print('z1 {}, shape {}'.format(z1, z1.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按上面的公式，当只有一个样本时，可以计算某个wj，比如w0的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w0 [0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gradient_w0 = (z1 - y1) * x1[0] # 与样本里面的第一个特征进行计算梯度\n",
    "print(f'gradient_w0 {gradient_w0}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样我们可以计算w1的梯度。，也就是第二个特征值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w1 [0.35485337]\n"
     ]
    }
   ],
   "source": [
    "gradient_w1 = (z1 - y1) * x1[1]\n",
    "print('gradient_w1 {}'.format(gradient_w1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聪明的读者可能已经想到，写一个for循环即可计算从w0到w12的所有权重的梯度，该方法读者可以自行实现。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3）使用Numpy进行梯度计算\n",
    "基于Numpy广播机制（对向量和矩阵计算如同对1个单一变量计算一样），可以更快速的实现梯度计算。得到的是一个13维的向量，每个分量分别代表该维度的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w_by_sample1 [0.         0.35485337 0.14478381 0.         0.62062832 1.13849828\n",
      " 1.26486811 0.53070911 0.         0.44860841 0.56625537 1.9714076\n",
      " 0.17679566], gradient.shape (13,)\n"
     ]
    }
   ],
   "source": [
    "gradient_w = (z1 - y1) * x1\n",
    "print('gradient_w_by_sample1 {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入数据中有多个样本，每个样本都对梯度有贡献。如上代码计算了只有样本1时的梯度值，同样的计算方法也可以计算样本2和样本3对梯度的贡献。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w_by_sample2 [4.95115308e-04 0.00000000e+00 5.50693832e-01 0.00000000e+00\n",
      " 3.62727044e-01 1.15004718e+00 1.64259797e+00 7.32343840e-01\n",
      " 9.12450018e-02 2.40970621e-01 1.16094704e+00 2.09863504e+00\n",
      " 4.29108324e-01], gradient.shape (13,)\n"
     ]
    }
   ],
   "source": [
    "# 计算第二个样本里面的每一个特征值的梯度\n",
    "x2 = x[1]\n",
    "y2 = y[1]\n",
    "z2 = net.forward(x2)\n",
    "gradient_w = (z2 - y2) * x2\n",
    "print('gradient_w_by_sample2 {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面计算梯度，一个样本里面 利用numpy的广播机制 可以同时计算13个特征的梯度，那么有没有这么一种可能，其实样本也可以批量的进行计算\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可能有的读者再次想到可以使用for循环把每个样本对梯度的贡献都计算出来，然后再作平均。但是我们不需要这么做，仍然可以使用Numpy的矩阵操作来简化运算，如3个样本的情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [[0.00000000e+00 1.80000000e-01 7.34418420e-02 0.00000000e+00\n",
      "  3.14814815e-01 5.77505269e-01 6.41606591e-01 2.69203139e-01\n",
      "  0.00000000e+00 2.27557411e-01 2.87234043e-01 1.00000000e+00\n",
      "  8.96799117e-02]\n",
      " [2.35922539e-04 0.00000000e+00 2.62405717e-01 0.00000000e+00\n",
      "  1.72839506e-01 5.47997701e-01 7.82698249e-01 3.48961980e-01\n",
      "  4.34782609e-02 1.14822547e-01 5.53191489e-01 1.00000000e+00\n",
      "  2.04470199e-01]\n",
      " [2.35697744e-04 0.00000000e+00 2.62405717e-01 0.00000000e+00\n",
      "  1.72839506e-01 6.94385898e-01 5.99382080e-01 3.48961980e-01\n",
      "  4.34782609e-02 1.14822547e-01 5.53191489e-01 9.87519166e-01\n",
      "  6.34657837e-02]], shape (3, 13)\n",
      "y [[0.42222222]\n",
      " [0.36888889]\n",
      " [0.66      ]], shape (3, 1)\n",
      "z [[2.39362982]\n",
      " [2.46752393]\n",
      " [2.02483479]], shape (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# 注意这里是一次取出3个样本的数据，不是取出第3个样本\n",
    "x3samples = x[0:3] # 获取前三个样本\n",
    "y3samples = y[0:3] # 获取前三个的真实值\n",
    "z3samples = net.forward(x3samples)\n",
    "\n",
    "print('x {}, shape {}'.format(x3samples, x3samples.shape))\n",
    "print('y {}, shape {}'.format(y3samples, y3samples.shape))\n",
    "print('z {}, shape {}'.format(z3samples, z3samples.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的x3samples, y3samples, z3samples的第一维大小均为3，表示有3个样本。下面计算这3个样本对梯度的贡献。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w [[0.00000000e+00 3.54853368e-01 1.44783806e-01 0.00000000e+00\n",
      "  6.20628319e-01 1.13849828e+00 1.26486811e+00 5.30709115e-01\n",
      "  0.00000000e+00 4.48608410e-01 5.66255375e-01 1.97140760e+00\n",
      "  1.76795660e-01]\n",
      " [4.95115308e-04 0.00000000e+00 5.50693832e-01 0.00000000e+00\n",
      "  3.62727044e-01 1.15004718e+00 1.64259797e+00 7.32343840e-01\n",
      "  9.12450018e-02 2.40970621e-01 1.16094704e+00 2.09863504e+00\n",
      "  4.29108324e-01]\n",
      " [3.21688482e-04 0.00000000e+00 3.58140452e-01 0.00000000e+00\n",
      "  2.35897372e-01 9.47722033e-01 8.18057517e-01 4.76275452e-01\n",
      "  5.93406432e-02 1.56713807e-01 7.55014992e-01 1.34780052e+00\n",
      "  8.66203097e-02]], gradient.shape (3, 13)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gradient_w = (z3samples - y3samples) * x3samples\n",
    "print('gradient_w {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么对于有N个样本的情形，我们可以直接使用如下方式计算出所有样本对梯度的贡献，这就是使用Numpy库广播功能带来的便捷。 小结一下这里使用Numpy库的广播功能：\n",
    "一方面可以扩展参数的维度，代替for循环来计算1个样本对从w0到w12的所有参数的梯度。另一方面可以扩展样本的维度，代替for循环来计算样本0到样本403对参数的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w shape (404, 13)\n",
      "[[0.00000000e+00 3.54853368e-01 1.44783806e-01 ... 5.66255375e-01\n",
      "  1.97140760e+00 1.76795660e-01]\n",
      " [4.95115308e-04 0.00000000e+00 5.50693832e-01 ... 1.16094704e+00\n",
      "  2.09863504e+00 4.29108324e-01]\n",
      " [3.21688482e-04 0.00000000e+00 3.58140452e-01 ... 7.55014992e-01\n",
      "  1.34780052e+00 8.66203097e-02]\n",
      " ...\n",
      " [7.66711387e-01 0.00000000e+00 3.35694398e+00 ... 3.87578270e+00\n",
      "  4.79373123e+00 2.45903597e+00]\n",
      " [4.83683601e-01 0.00000000e+00 3.14256160e+00 ... 3.62826605e+00\n",
      "  4.20149273e+00 2.30075782e+00]\n",
      " [1.42480820e+00 0.00000000e+00 3.58013213e+00 ... 4.13346610e+00\n",
      "  5.11244491e+00 2.54493671e+00]]\n"
     ]
    }
   ],
   "source": [
    "z = net.forward(x)\n",
    "gradient_w = (z - y) * x\n",
    "print('gradient_w shape {}'.format(gradient_w.shape))\n",
    "print(gradient_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w  (13,)\n",
      "w  (13, 1)\n",
      "[0.10197566 0.20327718 1.21762392 0.43059902 1.05326594 1.29064465\n",
      " 1.95461901 0.5342187  0.88702053 1.15069786 1.5790441  2.43714929\n",
      " 0.87116361]\n",
      "[[ 1.76405235]\n",
      " [ 0.40015721]\n",
      " [ 0.97873798]\n",
      " [ 2.2408932 ]\n",
      " [ 1.86755799]\n",
      " [-0.97727788]\n",
      " [ 0.95008842]\n",
      " [-0.15135721]\n",
      " [-0.10321885]\n",
      " [ 0.4105985 ]\n",
      " [ 0.14404357]\n",
      " [ 1.45427351]\n",
      " [ 0.76103773]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# axis = 0 表示把每一行做相加然后再除以总的行数\n",
    "gradient_w = np.mean(gradient_w, axis=0)\n",
    "print('gradient_w ', gradient_w.shape)\n",
    "print('w ', net.w.shape)\n",
    "print(gradient_w)\n",
    "print(net.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10197566],\n",
       "       [0.20327718],\n",
       "       [1.21762392],\n",
       "       [0.43059902],\n",
       "       [1.05326594],\n",
       "       [1.29064465],\n",
       "       [1.95461901],\n",
       "       [0.5342187 ],\n",
       "       [0.88702053],\n",
       "       [1.15069786],\n",
       "       [1.5790441 ],\n",
       "       [2.43714929],\n",
       "       [0.87116361]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = net.forward(x)\n",
    "gradient_w = (z - y) * x\n",
    "gradient_w = np.mean(gradient_w, axis=0)\n",
    "gradient_w = gradient_w[:, np.newaxis]\n",
    "gradient_w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述代码非常简洁地完成了w的梯度计算。同样，计算b的梯度的代码也是类似的原理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.599327274554706"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_b = (z - y)\n",
    "gradient_b = np.mean(gradient_b)\n",
    "# 此处b是一个数值，所以可以直接用np.mean得到一个标量\n",
    "gradient_b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上面计算w和b的梯度的过程，写成Network类的gradient函数，实现方法如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWork3(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        z = self.forward(x)\n",
    "        gradient_w = (z-y)*x\n",
    "        gradient_w = np.mean(gradient_w, axis=0)\n",
    "        gradient_w = gradient_w[:, np.newaxis]\n",
    "        gradient_b = (z - y)\n",
    "        gradient_b = np.mean(gradient_b)\n",
    "        \n",
    "        return gradient_w, gradient_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point [-100.0, -100.0], loss 7873.345739941161\n",
      "gradient [-45.87968288123223, -35.50236884482904]\n"
     ]
    }
   ],
   "source": [
    "# 调用上面定义的gradient函数，计算梯度\n",
    "# 初始化网络\n",
    "net = NetWork3(13)\n",
    "# 设置[w5, w9] = [-100., -100.]\n",
    "net.w[5] = -100.0\n",
    "net.w[9] = -100.0\n",
    "\n",
    "z = net.forward(x)\n",
    "loss = net.loss(z, y)\n",
    "gradient_w, gradient_b = net.gradient(x, y)\n",
    "gradient_w5 = gradient_w[5][0]\n",
    "gradient_w9 = gradient_w[9][0]\n",
    "print('point {}, loss {}'.format([net.w[5][0], net.w[9][0]], loss))\n",
    "print('gradient {}'.format([gradient_w5, gradient_w9]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （5）代码封装Train函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network4(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights,1)\n",
    "        self.w[5] = -100.\n",
    "        self.w[9] = -100.\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        z = self.forward(x)\n",
    "        gradient_w = (z-y)*x\n",
    "        gradient_w = np.mean(gradient_w, axis=0)\n",
    "        gradient_w = gradient_w[:, np.newaxis]\n",
    "        gradient_b = (z - y)\n",
    "        gradient_b = np.mean(gradient_b)        \n",
    "        return gradient_w, gradient_b\n",
    "    \n",
    "    def update(self, gradient_w5, gradient_w9, eta=0.01):\n",
    "        net.w[5] = net.w[5] - eta * gradient_w5\n",
    "        net.w[9] = net.w[9] - eta * gradient_w9\n",
    "        \n",
    "    def train(self, x, y, iterations=100, eta=0.01):\n",
    "        points = []\n",
    "        losses = []\n",
    "        for i in range(iterations):\n",
    "            points.append([net.w[5][0], net.w[9][0]])\n",
    "            z = self.forward(x)\n",
    "            L = self.loss(z, y)\n",
    "            gradient_w, gradient_b = self.gradient(x, y)\n",
    "            gradient_w5 = gradient_w[5][0]\n",
    "            gradient_w9 = gradient_w[9][0]\n",
    "            self.update(gradient_w5, gradient_w9, eta)\n",
    "            losses.append(L)\n",
    "            if i % 50 == 0:\n",
    "                print('iter {}, point {}, loss {}'.format(i, [net.w[5][0], net.w[9][0]], L))\n",
    "        return points, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, point [-99.54120317118768, -99.64497631155172], loss 7873.345739941161\n",
      "iter 50, point [-78.9761810944732, -83.65939206734069], loss 5131.480704109405\n",
      "iter 100, point [-62.4493631356931, -70.67918223434114], loss 3346.754494352463\n",
      "iter 150, point [-49.17799206644332, -60.12620415441553], loss 2184.906016270654\n",
      "iter 200, point [-38.53070194231174, -51.533984751788346], loss 1428.4172504483342\n",
      "iter 250, point [-29.998249130283174, -44.52613603923428], loss 935.7392894242679\n",
      "iter 300, point [-23.169901624519575, -38.79894318028118], loss 614.7592258739251\n",
      "iter 350, point [-17.71439280083778, -34.10731848231335], loss 405.53408184471505\n",
      "iter 400, point [-13.364557220746388, -30.253470630210863], loss 269.0551396220099\n",
      "iter 450, point [-9.904936677384967, -27.077764259976597], loss 179.9364750604248\n",
      "iter 500, point [-7.161782280775628, -24.451346444229817], loss 121.65711285489998\n",
      "iter 550, point [-4.994989383373879, -22.270198517465555], loss 83.46491706360901\n",
      "iter 600, point [-3.2915916915280783, -20.450337700789422], loss 58.36183370758033\n",
      "iter 650, point [-1.9605131425212885, -18.923946252536773], loss 41.792808952534\n",
      "iter 700, point [-0.9283343968114077, -17.636248840494844], loss 30.792614998570482\n",
      "iter 750, point [-0.13587780041668718, -16.542993494033716], loss 23.43065354742935\n",
      "iter 800, point [0.4645474092373408, -15.60841945615185], loss 18.449664464381506\n",
      "iter 850, point [0.9113672926170796, -14.803617811655524], loss 15.030615923519784\n",
      "iter 900, point [1.2355357562745004, -14.105208963393421], loss 12.639705730905764\n",
      "iter 950, point [1.4619805189121953, -13.494275706622066], loss 10.928795653764196\n",
      "iter 1000, point [1.6107694974712377, -12.955502492189021], loss 9.670616807081698\n",
      "iter 1050, point [1.6980516626374353, -12.476481020835202], loss 8.716602071285436\n",
      "iter 1100, point [1.7368159644039771, -12.04715001603925], loss 7.969442965176621\n",
      "iter 1150, point [1.7375034995020395, -11.659343238414994], loss 7.365228465612388\n",
      "iter 1200, point [1.7085012931271857, -11.306424818680442], loss 6.861819342703047\n",
      "iter 1250, point [1.6565405824483015, -10.982995030930885], loss 6.431280353078019\n",
      "iter 1300, point [1.5870180647823104, -10.684652890749808], loss 6.054953198278096\n",
      "iter 1350, point [1.5042550040699705, -10.407804594738165], loss 5.720248083137862\n",
      "iter 1400, point [1.4117062100403601, -10.14950894127009], loss 5.418553777303124\n",
      "iter 1450, point [1.3121285818148223, -9.907352585055445], loss 5.143875665274019\n",
      "iter 1500, point [1.2077170340724794, -9.67934935975478], loss 4.891947653805328\n",
      "iter 1550, point [1.1002141124777076, -9.463859017459276], loss 4.659652555766873\n",
      "iter 1600, point [0.990998385834045, -9.259521632951046], loss 4.444643323159747\n",
      "iter 1650, point [0.8811557188942747, -9.065204645952335], loss 4.245095084874306\n",
      "iter 1700, point [0.7715367363576023, -8.87996009965401], loss 4.059542401818773\n",
      "iter 1750, point [0.662803148565214, -8.702990105791185], loss 3.88677206759292\n",
      "iter 1800, point [0.5554650931141796, -8.533618947271485], loss 3.7257521401326525\n",
      "iter 1850, point [0.4499112301277286, -8.371270536496699], loss 3.5755846299900256\n",
      "iter 1900, point [0.3464329929523944, -8.215450195281456], loss 3.435473657404253\n",
      "iter 1950, point [0.24524412503452966, -8.065729922139326], loss 3.3047037451160453\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 获取数据\n",
    "train_data, test_data = load_data()\n",
    "x = train_data[:, :-1]\n",
    "y = train_data[:, -1:]\n",
    "# 创建网络\n",
    "net = Network4(13)\n",
    "num_iterations=2000\n",
    "# 启动训练\n",
    "points, losses = net.train(x, y, iterations=num_iterations, eta=0.01)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# 画出损失函数的变化趋势\n",
    "plot_x = np.arange(num_iterations)\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （7）随机梯度下降法（ Stochastic Gradient Descent）\n",
    "在上述程序中，每次损失函数和梯度计算都是基于数据集中的全量数据。对于波士顿房价预测任务数据集而言，样本数比较少，只有404个。但在实际问题中，数据集往往非常大，如果每次都使用全量数据进行计算，效率非常低，通俗地说就是“杀鸡焉用牛刀”。由于参数每次只沿着梯度反方向更新一点点，因此方向并不需要那么精确。一个合理的解决方案是每次从总的数据集中随机抽取出小部分数据来代表整体，基于这部分数据计算梯度和损失来更新参数，这种方法被称作随机梯度下降法（Stochastic Gradient Descent，SGD）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
